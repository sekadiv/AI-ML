{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "MBA_Assignment_Notebook.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sekadiv/AI-ML/blob/main/MBA_Assignment_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAH--6TZUSNZ"
      },
      "source": [
        "# Market Basket Analysis using Apriori Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CWLq48WUSNb"
      },
      "source": [
        "As part of this assignment, you are expected to fill the blanks in the code provided in the Jupyter Notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yduX0UeSUSNc"
      },
      "source": [
        "# Defining the environment variables \n",
        "\n",
        "import os\n",
        "import sys\n",
        "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_161/jre\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/home/ec2-user/spark-2.4.4-bin-hadoop2.7\"\n",
        "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
        "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
        "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGvI9JwvUwPg",
        "outputId": "be57de8c-7823-493a-f2b7-ecfe070a4c2b"
      },
      "source": [
        "pip install pyspark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 68kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 51.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=f0d553a84a898419ef6026a48ec32959f7f02ef3151070e0f856e63a1def5a46\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30wtZBSpUSNd"
      },
      "source": [
        "# Importing the SparkSession library\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "MAX_MEMORY = \"5g\"\n",
        "\n",
        "# Creating the SparkSession object\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "                    .appName('apriori')\\\n",
        "                    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
        "                    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
        "                    .getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "dMZcxyUDUSNe",
        "outputId": "cc410f61-89c4-48bb-8675-9df41b3d714d"
      },
      "source": [
        "spark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://57cb394a92d9:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>apriori</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f246ae343d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukOiuRcKUSNe",
        "outputId": "7af2fd7c-0d74-43bb-d256-17298b698ede"
      },
      "source": [
        "# Loading the data from Market_Basket_Optimisation.csv inside the dataframe\n",
        "transaction_df = spark.read.csv(\"transactions.csv\")\n",
        "\n",
        "# Schema of the dataframe\n",
        "transaction_df.printSchema()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            " |-- _c2: string (nullable = true)\n",
            " |-- _c3: string (nullable = true)\n",
            " |-- _c4: string (nullable = true)\n",
            " |-- _c5: string (nullable = true)\n",
            " |-- _c6: string (nullable = true)\n",
            " |-- _c7: string (nullable = true)\n",
            " |-- _c8: string (nullable = true)\n",
            " |-- _c9: string (nullable = true)\n",
            " |-- _c10: string (nullable = true)\n",
            " |-- _c11: string (nullable = true)\n",
            " |-- _c12: string (nullable = true)\n",
            " |-- _c13: string (nullable = true)\n",
            " |-- _c14: string (nullable = true)\n",
            " |-- _c15: string (nullable = true)\n",
            " |-- _c16: string (nullable = true)\n",
            " |-- _c17: string (nullable = true)\n",
            " |-- _c18: string (nullable = true)\n",
            " |-- _c19: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaOQxb7LUSNf",
        "outputId": "7bcc2cfe-349a-40a3-cf77-559bc7f53150"
      },
      "source": [
        "# Print the first 20 transactions of the dataframe\n",
        "transaction_df.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+------------+-------------+----------------+-------------+----------------+--------------+--------------+------------+--------------------+--------------+---------+-----+-----+-------------+------+-----------------+---------------+-------+---------+\n",
            "|              _c0|         _c1|          _c2|             _c3|          _c4|             _c5|           _c6|           _c7|         _c8|                 _c9|          _c10|     _c11| _c12| _c13|         _c14|  _c15|             _c16|           _c17|   _c18|     _c19|\n",
            "+-----------------+------------+-------------+----------------+-------------+----------------+--------------+--------------+------------+--------------------+--------------+---------+-----+-----+-------------+------+-----------------+---------------+-------+---------+\n",
            "|           shrimp|     almonds|      avocado|  vegetables mix| green grapes|whole weat flour|          yams|cottage cheese|energy drink|        tomato juice|low fat yogurt|green tea|honey|salad|mineral water|salmon|antioxydant juice|frozen smoothie|spinach|olive oil|\n",
            "|          burgers|   meatballs|         eggs|            null|         null|            null|          null|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|          chutney|        null|         null|            null|         null|            null|          null|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|           turkey|     avocado|         null|            null|         null|            null|          null|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|    mineral water|        milk|   energy bar|whole wheat rice|    green tea|            null|          null|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|   low fat yogurt|        null|         null|            null|         null|            null|          null|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|whole wheat pasta|french fries|         null|            null|         null|            null|          null|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|             soup| light cream|      shallot|            null|         null|            null|          null|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|frozen vegetables|   spaghetti|    green tea|            null|         null|            null|          null|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|     french fries|        null|         null|            null|         null|            null|          null|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|             eggs|    pet food|         null|            null|         null|            null|          null|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|          cookies|        null|         null|            null|         null|            null|          null|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|           turkey|     burgers|mineral water|            eggs|  cooking oil|            null|          null|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|        spaghetti|   champagne|      cookies|            null|         null|            null|          null|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|    mineral water|      salmon|         null|            null|         null|            null|          null|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|    mineral water|        null|         null|            null|         null|            null|          null|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|           shrimp|   chocolate|      chicken|           honey|          oil|     cooking oil|low fat yogurt|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|           turkey|        eggs|         null|            null|         null|            null|          null|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|           turkey|  fresh tuna|     tomatoes|       spaghetti|mineral water|       black tea|        salmon|          eggs|     chicken|extra dark chocolate|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "|        meatballs|        milk|        honey|    french fries|  protein bar|            null|          null|          null|        null|                null|          null|     null| null| null|         null|  null|             null|           null|   null|     null|\n",
            "+-----------------+------------+-------------+----------------+-------------+----------------+--------------+--------------+------------+--------------------+--------------+---------+-----+-----+-------------+------+-----------------+---------------+-------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDgIgWTrUSNf"
      },
      "source": [
        "## Apriori Algorithm\n",
        "\n",
        "You are already familiar with the Apriori algorithm. The points below summarise the entire algorithm for revision purposes.\n",
        "\n",
        "The Apriori algorithm mainly involves two parts:\n",
        "<br> \n",
        "- Part A: Frequent Itemset Generation\n",
        "<br> \n",
        "- Part B: Rule Generation\n",
        "\n",
        "Frequent Itemset Generation in Apriori Algorithm:\n",
        "\n",
        "![img%20-%20-0.jpg](attachment:img%20-%20-0.jpg)\n",
        "\n",
        "- The first step of the algorithm is to identify distinct items in the given set of transactions. Let’s say these are ({A}, {B}, {C}, {D}).\n",
        "\n",
        "- Once you have different items, your next step would be to calculate the support of each of these items. Items with support values less than the minimum support are removed from the distinct items list.\n",
        "\n",
        "- The next step is to create higher-order itemsets by merging the existing itemsets. This can be done using the candidate generation technique. We will cover this concept in detail while implementing the generation of higher item sets on code.\n",
        "\n",
        "- Using the 1 itemsets ({A}, {B}, {C}, {D}) and assuming that only A, B and C are frequent, we generate itemsets {A, B}, {A, C} and {B, C}. Note that none of the 2-item sets contains the item D. This is because we have applied the Apriori principle. Since D is infrequent, any item set containing D, e.g., {A, D}, {C, D} and {B, C, D} will automatically become infrequent.\n",
        "\n",
        "- Once you have the higher-order itemsets, you can calculate the support for these item sets and again remove the itemsets that do not qualify the minimum support criteria.\n",
        "\n",
        "- This ( n-1 ) -item sets then become inputs for the generation of n-item sets, and once again the item sets that do not satisfy the minimum support criteria are removed. This process continues until no new itemsets can be generated.\n",
        "\n",
        " \n",
        "\n",
        "Rule Generation in the Apriori Algorithm:\n",
        "\n",
        "- Once you have all the frequent itemsets, we can proceed with the rule generation process. We begin with 2-itemsets and generate all the possible rules.\n",
        "\n",
        "- For each rule, we check the corresponding confidence value and return the rule only if its confidence is above the minimum confidence level.\n",
        "\n",
        "- In order to avoid generating redundant rules, we utilise confidence-based pruning. Using this, we eliminate the generation of higher-order rules if their corresponding lower-order rules are infrequent. This portion will be explained in more detail in the code demonstration of rule generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzNa3fMjUSNg"
      },
      "source": [
        "### Part A. Frequent Itemset generation\n",
        "\n",
        "This part of the algoritm is divided as follows:\n",
        "\n",
        "- Extracting unique items\n",
        "\n",
        "- Computing the support value for individual itemsets\n",
        "\n",
        "- Generating higher-order itemsets\n",
        "\n",
        "- Combining the functions above to generate all the frequent itemsets\n",
        "\n",
        "\n",
        "We have defined different functions to perfrom the tasks mentioned above. All the functions are called under the main function **'apriori()'** that is defined in the later parts of the notebook. You can refer to the function to understand the purpose of each function and how they are called to perform the required task.\n",
        "\n",
        "Let's see each of them one by one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtProzovUSNg"
      },
      "source": [
        "Functions defined:\n",
        " - generate_unique_item_set() - To generate the dataFrame that holds all the unique items in the transaction base in a single column\n",
        " \n",
        "![img%20-%201.jpg](attachment:img%20-%201.jpg)\n",
        " \n",
        " - remove_duplicate_columns() - Remove the columns with duplicate values\n",
        " \n",
        "![img%20-%202.jpg](attachment:img%20-%202.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDA8wCUsUSNh"
      },
      "source": [
        "# Function to remove the columns with duplicate values\n",
        "\n",
        "def remove_duplicate_columns(x):\n",
        "    # Length of the column\n",
        "    col_len = len(x)\n",
        "    \n",
        "    # Empty RDD - set of values\n",
        "    columns = set()\n",
        "    \n",
        "    # Removing any additional spaces from the elements and adding the elements into the column from RDD 'x'\n",
        "    for col in range(col_len):\n",
        "        x_col = str(x[col]).strip()\n",
        "        columns.add(x_col)\n",
        "    \n",
        "    # To check if elements are present in the provided dataframe/RDD \n",
        "    if len(columns) < col_len:\n",
        "        return []\n",
        "    \n",
        "    # Returning the sorted list of items in each element as tuple\n",
        "    return [(tuple(sorted(columns)))]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45p9OkEXUSNh"
      },
      "source": [
        "#### Reasoning\n",
        "What is the use of strip() funciton in defining the remove_duplicate_columns() function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20TglBVoUSNh"
      },
      "source": [
        "(Write your answer here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3a0mLnwUSNi"
      },
      "source": [
        "# For the given dataset writing a function to return the list of distinct items in the dataset\n",
        "\n",
        "def generate_unique_item_set(df):\n",
        "    # empty dataframe\n",
        "    total_item_set_df = None\n",
        "    \n",
        "    # Iteration over each column - 20 columns\n",
        "    for col_index in range(20):\n",
        "        \n",
        "        # Loading the elements of each column individually\n",
        "        _c_df = df.select(\"_c\" + str(col_index))\n",
        "        \n",
        "        if total_item_set_df is None:\n",
        "            # None for the first iteration in the loop\n",
        "            total_item_set_df = _c_df\n",
        "            \n",
        "        else:\n",
        "            # After the first iteration, appending the entries from each column to total_item_set_df\n",
        "            total_item_set_df = total_item_set_df.union(_c_df)\n",
        "            \n",
        "    # Return Value: Dataframe with unique items (no repetition) and null values removed from the dataFrame        \n",
        "    \n",
        "    # df.na provides all the null values; all the null values must be dropped\n",
        "    # .rdd converts the DataFrame to RDD\n",
        "    # remove_duplicate_columns must be applied to elements of RDD such that each item in transaction is a separate element \n",
        "    # Remember that the function 'remove_duplicate_columns' is applied to each element of the RDD, in short, every row of the dataframe should be passed into it. \n",
        "    \n",
        "    return total_item_set_df.select(\"_c0\").na.drop().rdd.flatMap(remove_duplicate_columns).distinct().toDF()"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "namh5-4CUSNi"
      },
      "source": [
        "#### Reasoning\n",
        "\n",
        "Can you tell why only the column '\\_c0' is extracted from the function above? Provide your answer below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_KFE4LVUSNi"
      },
      "source": [
        "(Write your answer here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUFpwXH2USNj"
      },
      "source": [
        "<br> Use the above function **generate_unique_item_set** and the dataframe **transaction_df** to create the dataFrame **item_set** that stores the the name of all the items in the transaction base as independent elements. <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBi9a0gcUSNj"
      },
      "source": [
        "# Syntax structure: item_sets = function(dataframe)\n",
        "\n",
        "item_sets = generate_unique_item_set(transaction_df)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhqSJRKFUSNj",
        "outputId": "89b97909-828c-4096-e6dc-979fc564e562"
      },
      "source": [
        "# Print the first twenty rows of the item_sets dataframe\n",
        "item_sets.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|                  _1|\n",
            "+--------------------+\n",
            "|   whole wheat pasta|\n",
            "|           asparagus|\n",
            "|            pancakes|\n",
            "|         blueberries|\n",
            "|            zucchini|\n",
            "|              shrimp|\n",
            "|             burgers|\n",
            "|           spaghetti|\n",
            "|         french wine|\n",
            "|       strong cheese|\n",
            "|extra dark chocolate|\n",
            "|              melons|\n",
            "|               cream|\n",
            "|   frozen vegetables|\n",
            "|           meatballs|\n",
            "|          energy bar|\n",
            "|            escalope|\n",
            "|        energy drink|\n",
            "|                mint|\n",
            "|      vegetables mix|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_hF5SrVUSNj",
        "outputId": "e7d064e2-dd03-45e0-bd0d-0e3c5af30f16"
      },
      "source": [
        "# Give the number of unique items in the transaction dataset\n",
        "item_sets.count()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "119"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3qUWoAcUSNk"
      },
      "source": [
        "<br>The itemset should contain all the unique items in the dataframe after you have removed all the duplicate values. Now, this dataframe will be helpful in creating the frequent item set ahead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a64kTnftUSNk"
      },
      "source": [
        "__________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QUHr8lgUSNk"
      },
      "source": [
        "Now, you will have the first order candidate itemset inside a dataframe. This dataframe will be used in generating the frequent itemset of different orders. Following functions have been defined to help you obtain the frequent itemset:\n",
        "\n",
        " - filter_and_map_transaction()\n",
        " - get_all_possible_candidate_sets()\n",
        " - get_freq_item_sets()\n",
        " - is_freq_item_set_not_empty()\n",
        " - apriori()\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zti2OHSfUSNk"
      },
      "source": [
        "<br>**filter_and_map_transaction()**\n",
        "<br> <br>The function is expected to compare each item sold by the company with the candidate set. If the item is present in the transaction, the corresponding item in the candidate set must be mapped with value 1 and if they are absent, then the value must be 0.\n",
        "\n",
        "Refer to the image provided after the function definition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GHJKnD_USNk"
      },
      "source": [
        "def filter_and_map_transaction(x, candidate_set_shared):\n",
        "    \n",
        "    c_k = []\n",
        "    \n",
        "    rows = len(candidate_set_shared.value)\n",
        "    cols = len(candidate_set_shared.value[0])\n",
        "    \n",
        "    # Checking each transaction\n",
        "    for row_i in range(rows):\n",
        "        item_set = set()\n",
        "        for col_i in range(cols):\n",
        "            item_set.add(candidate_set_shared.value[row_i][col_i])\n",
        "        \n",
        "        # Map the condition with the correct value (1/0)\n",
        "        if item_set.issubset(set(x)):\n",
        "            c_k.append((candidate_set_shared.value[row_i], 1))\n",
        "        else:\n",
        "            c_k.append((candidate_set_shared.value[row_i], 0))\n",
        "    return c_k"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aigJTE6vUSNl"
      },
      "source": [
        "Sample: The items from the first transaction are displayed along with the function. You can check how the values are mapped with 1 and 0.\n",
        "\n",
        "![img%20-%203.jpg](attachment:img%20-%203.jpg)\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O00dM4NVUSNl"
      },
      "source": [
        "**get_all_possible_candidate_sets()**\n",
        "<br> <br>The function is used to generate itemsets of higher order from the first order. The value 'k+1' denotes the order number and the function creates itemsets of order 'k+1' by merging the two input itemset of order k & 1 respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svP-mNCyUSNl"
      },
      "source": [
        "def get_all_possible_candidate_sets(candidate_item_sets_k, candidate_item_sets_0):\n",
        "    \n",
        "    # Convert the elements of the candidate_item_sets_k from the list format into tuple\n",
        "    # You can refer to the commands below to check why only the first element is used for conversion\n",
        "    candidate_item_sets_k = candidate_item_sets_k.map(lambda x: tuple(x[0])).toDF()\n",
        "    # toDF() converts the rdd into a dataFrame\n",
        "    \n",
        "    # Returning the k+1 order\n",
        "    return candidate_item_sets_k.crossJoin(candidate_item_sets).rdd.flatMap(remove_duplicate_columns).distinct()\n",
        "    # crossJoin will help to combine one element of one dataFrame with all the elements of another dataFrame"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTkSddpHUSNm"
      },
      "source": [
        "#### Reasoning\n",
        "\n",
        "What is the purpose of using the function crossjoin()? You can check the documentation to understand the output of the function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibooU1QdUSNm"
      },
      "source": [
        "(Write your answer here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF8QavdTUSNm"
      },
      "source": [
        "<br>**get_freq_item_sets()**\n",
        "<br> <br>The function is used to generate filtered itemsets from the provided dataFrame based on the minimum support value. It filters candidates sets by the minimum support set by main Apriori function. Output of this should be repeatedly added in a array to generate the final output of main apriori function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvbgUzCPUSNm"
      },
      "source": [
        "# Funciton to generate frequent itemset\n",
        "\n",
        "def get_freq_item_sets(total_records, candidate_sets_shared, transaction_df_rdd, min_support):\n",
        "\n",
        "    \"\"\"\n",
        "    Attributes\n",
        "    ----------\n",
        "    total_records: Total number of records in the dataFrame\n",
        "    \n",
        "    candidate_sets_shared: List of items in the transaction base\n",
        "    \n",
        "    transaction_df_rdd: Transactions dataFrame converted into an RDD\n",
        "    \n",
        "    min_support: Minimum support threshold\n",
        "    ----------\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    filtered_item_set = transaction_df_rdd.flatMap(lambda x: filter_and_map_transaction(x, candidate_sets_shared)) \\\n",
        "                                          \n",
        "                                          # Sample output from filter_and_map_transaction function can be seen above\n",
        "                                          # Now complete the function to generate the frequent item set\n",
        "                                          # support value = Frequency of the item in the transactions/total number of transactions\n",
        "                                          \n",
        "                                          # 1. function to calculate frequency of each item in the transaction base\n",
        "                                          .______________________ \\\n",
        "                                          \n",
        "                                          # 2. funciton to calculate the support value (formula in comments above) \n",
        "                                          # You are expected to provide the total number of records when you call the function.\n",
        "                                          .______________________ \\\n",
        "                                          \n",
        "                                          # 3. funciton to filter the items that have support value greater than the min_support\n",
        "                                          # You are expected to provide the min_support when you call the function.\n",
        "                                          .______________________\n",
        "    \n",
        "    return filtered_item_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN5CTZvpnMKX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTxXjvv2USNm"
      },
      "source": [
        "<br>**is_freq_item_set_not_empty()**\n",
        "<br> <br>The function is used to check if the frequent itemset has any transactions or not.\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG26U-brUSNn"
      },
      "source": [
        "# Function to check if \"freq_item_sets\" has relevant values (Not empty and all the values are not None)\n",
        "\n",
        "# The first condition is defined to check if there are elements in the itemset. \n",
        "# There might be a case that elements are present but all of them are none.\n",
        "# The second condition checks if all the elements of the itemset are not none.\n",
        "\n",
        "def is_freq_item_set_not_empty(freq_item_sets):\n",
        "    return (freq_item_sets.count() > 0) _____ (freq_item_sets is not None)\n",
        "    \n",
        "    # Provide the correct logical operator in the condition above "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRB2-F8qUSNn"
      },
      "source": [
        "<br>**apriori()**\n",
        "<br> <br>This is the main function and is used to return all frequent item sets along with their support values. It should return an array of Spark dataFrames. \n",
        "\n",
        " - Array index should indicate the order of the frequent item set. For example data frame at index i should contain frequent item sets of order (i + 1).\n",
        " - Uses all the functions defined above to provide the results.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOLP9Pf2USNn"
      },
      "source": [
        "from pyspark.sql.types import StructType, ArrayType, StructField, DoubleType, StringType\n",
        "\n",
        "def apriori(item_sets, transaction_df_rdd, min_support):\n",
        "    \n",
        "    \"\"\"\n",
        "    Attributes\n",
        "    ----------\n",
        "    item_sets: DataFrame that has all the items present in the transactions\n",
        "    \n",
        "    transaction_df_rdd: Transacations in the form of an RDD\n",
        "    \n",
        "    min_support: Minimum support threshold\n",
        "    -----------\n",
        "    \"\"\"\n",
        "    \n",
        "    # Calculate the total number of transactions in the dataset and store the count in total_records\n",
        "    total_records = ________________\n",
        "\n",
        "    # Defining a blank list that will store the frequent itemsets\n",
        "    freq_item_sets_all_orders = []\n",
        "\n",
        "    # Candidate sets of order 1 will be the complete item list from the transactions\n",
        "    # As you can see, broadcast function is used here. It is used to broadcast a variable on all the executors.\n",
        "    candidate_sets_order_1 = spark.sparkContext.broadcast(item_sets.collect())\n",
        "    \n",
        "    # Complete the function to generate the filtered item set of order 1. Check the function definition of 'get_freq_item_sets' to understand the attributes.\n",
        "    frequent_item_sets_order_1 = get_freq_item_sets(____, _____, _____, min_support)\n",
        "    \n",
        "    # Appending the results of frequent_item_sets_order_1 in the freq_item_sets_all_orders\n",
        "    freq_item_sets_all_orders.__________(frequent_item_sets_order_1)\n",
        "    \n",
        "    # Convert the elements of the rdd 'frequent_item_sets_order_1' into a dataFrame with each element as tuple\n",
        "    frequent_item_sets_order_1_df = frequent_item_sets_order_1.map(lambda x:___________).toDF()\n",
        "    \n",
        "    # Generating higher order rules\n",
        "    k = 0\n",
        "    \n",
        "    # Loop will run till higher order item sets can be generated\n",
        "    while is_freq_item_set_not_empty(freq_item_sets_all_orders[k]):\n",
        "        # Generating candidate sets of order k+1 \n",
        "        current_candidate_sets = get_all_possible_candidate_sets(freq_item_sets_all_orders[k], frequent_item_sets_order_1_df)\n",
        "        \n",
        "        # Broadcasting candidate sets\n",
        "        current_candidate_sets = spark.sparkContext.broadcast(current_candidate_sets.collect())\n",
        "        \n",
        "        # Filtering candidate sets to get the frequent item sets of order 'k+1' \n",
        "        current_frequent_item_sets = get_freq_item_sets(total_records, current_candidate_sets, transaction_df_rdd, min_support)\n",
        "        \n",
        "        # Appending the list 'freq_item_sets_all_orders' with the frequent itemset of order k+1\n",
        "        freq_item_sets_all_orders.append(current_frequent_item_sets)\n",
        "        \n",
        "        # freq_item_sets_all_orders is a list of RDDs where the element k stores the frequent item set of order k+1  \n",
        "        \n",
        "        # increasing k by 1\n",
        "        k += 1\n",
        "    \n",
        "    \n",
        "    return freq_item_sets_all_orders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh5KPw0GUSNn"
      },
      "source": [
        "All the functions that we have created until now will come into play in the Apriori function. All the possible k-order itemsets will be generated, and the support will be calculated for each itemset. The freqItemsets of all orders will be stored\n",
        "and returned once the function is called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9DWoXdwUSNo"
      },
      "source": [
        "# Generate the frequent item set using the apriori function created above.\n",
        "# Minimum support = 0.01\n",
        "# Check the format in which each attribute must be provided\n",
        "\n",
        "freq_item_sets_all_orders = \n",
        "\n",
        "# freq_item_sets_all_orders is a list of RDDs where the element k stores the frequent item set of order k+1\n",
        "# Print freq_item_sets_all_orders to check the structure\n",
        "\n",
        "print(freq_item_sets_all_orders)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZMRNtv2USNo"
      },
      "source": [
        "The rules for each order are now stored in the following list: **freq_item_sets_all_orders**\n",
        "\n",
        "Now, print the support value for every frequent itemset order in the form of a dataFrame. The sample outputs have been provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaqHgZ9lUSNo"
      },
      "source": [
        "# First order itemsets that have support value greater than the threshold (Print 20 rows)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2GRuP3XUSNo"
      },
      "source": [
        "![img%20-%204.JPG](attachment:img%20-%204.JPG)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MecaxitUSNo"
      },
      "source": [
        "# Second order itemsets that have support value greater than the threshold (Print 20 rows)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYnWNEaoUSNo"
      },
      "source": [
        "![img%20-%205.JPG](attachment:img%20-%205.JPG)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "k4Yth3NwUSNp"
      },
      "source": [
        "# Third order itemsets that have support value greater than the threshold (Print 20 rows)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua3904gLUSNp"
      },
      "source": [
        "![img%20-%206.JPG](attachment:img%20-%206.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmi1vYocUSNp"
      },
      "source": [
        "Congratulations! You now have your frequent itemsets. Let’s move on to Rule Generation. Hope you had no trouble\n",
        "in completing the codes until now. If you have any doubts regarding the functions or commands used, you can keep a note of them and discuss in one of the live sessions.\n",
        "\n",
        "The next part of the assignment is hard and involves patience to understand. You must try to understand the flow of the defined functions and should be able to understand what each function is doing.\n",
        "\n",
        "\n",
        "### B. Rule generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwHjDHyuUSNp"
      },
      "source": [
        "Let us refresh our memory on the properties of rules. \n",
        "\n",
        "Given the rule {A ---> B, C}:\n",
        "- A is the antecedent\n",
        "- (B, C) is the consequent\n",
        "\n",
        "<br>\n",
        "In mathematical terms, confidence is defined as: \n",
        "\n",
        "**Confidence = support(A, B, C) / support(A)**\n",
        "\n",
        "<br>\n",
        "Notice that {A, B, C} is the frequent itemset and A is the antecedent. Hence, in the set format, we can write the formula as:\n",
        "\n",
        "**Confidence = support(Frequent Item Set) / support(Frequent Item Set - Consequent)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bgje3Q2uUSNp"
      },
      "source": [
        "Now, let's try to generate the confidence score. \n",
        "\n",
        "- The code for generating the rules is tough. Therefore, you are provided only with few blanks to fill based on the understanding of Spark and logic used.\n",
        "- Due to the limitations of Spark, the code provides the confidence scores for single antecedant only. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikxgyjzhUSNq"
      },
      "source": [
        "To calculate the confidence score, you will have to create subsets from the frequent item set to check all the possible combinations which may result in a higher confidence value than the threshold. Once you have all the subsets, you can use the support value associated with each subset to find the confidence score.\n",
        "\n",
        "As the first step, we will create the subsets.\n",
        "\n",
        "**powerset()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr82SdOAUSNq"
      },
      "source": [
        "# Function to generate all possible subsets from a set\n",
        "# You can google powerset if you want to read more about it\n",
        "\n",
        "def powerset(s):\n",
        "    slist = list(s)\n",
        "    result = [[]]\n",
        "    for x in slist:\n",
        "        \n",
        "        # Here, you may want to explore what the purpose of function 'extend' is\n",
        "        # Note that it is a reccursive function that adds elements to a list. It is similar to the extend() function in Python.\n",
        "        result.extend([subset + [x] for subset in result])\n",
        "        \n",
        "    return [item_set for item_set in result if (len(item_set) > 0 and len(item_set) < len(slist))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mZeJH9dUSNq"
      },
      "source": [
        "#### Reasoning\n",
        "\n",
        "Can you use the function append() in place of the function extend() in the above code?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKoXZYaxUSNq"
      },
      "source": [
        "(Write your answer here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNbRqEOTUSNq"
      },
      "source": [
        "<br>**generate_rules_with_confidence**\n",
        "\n",
        "The function will help you generate rules with the confidence score \n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lyjCKv7USNq"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "def generate_rules_with_confidence(x, k, freq_item_sets_map_all_orders_shared, min_confidence):\n",
        "    \n",
        "    \"\"\"\n",
        "    Attributes\n",
        "    ----------\n",
        "    \n",
        "    x: freq_item_sets_all_orders; list that stores the frequent item set with the confidence score\n",
        "    \n",
        "    k: order of the frequent item set\n",
        "    \n",
        "    freq_item_sets_map_all_orders_shared: broadcasted map (key-value pair) of frequent item sets where \n",
        "    - key is the subset and \n",
        "    - value is the corresponding support\n",
        "    \n",
        "    min_confidence: Threshold confidence value \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # first column of the RDD is extracted as a frequent item set\n",
        "    freq_item_set = set(x[0])\n",
        "    \n",
        "    # all_subsets stores all the subsets from the freq_item_set\n",
        "    all_subsets = powerset(freq_item_set)\n",
        "    \n",
        "    # Defining an empty list to store the rules\n",
        "    rules = []\n",
        "    \n",
        "    # Converting the broadcasted values in the required format (elements as tuple)\n",
        "    freq_item_set_support = freq_item_sets_map_all_orders_shared[k].value[tuple(sorted(freq_item_set))]\n",
        "    \n",
        "    # Generated rules contain single element as antecendent. The reason is we used cross join for generating candidate\n",
        "    # itemsets instead of doing or operation of two candidate sets as the for the later completely distributed approach\n",
        "    # cannot be implemented.\n",
        "    \n",
        "    candidate_set_key = ''\n",
        "    subset_k = 0\n",
        "    for subset in all_subsets:\n",
        "        antecedent = set(subset)\n",
        "        \n",
        "        # Consequent is generated by removing the antecedent from the frequent item set\n",
        "        consequent = freq_item_set - set(antecedent)\n",
        "    \n",
        "        # Different calculation when there is a single element in consequent\n",
        "        if (len(set(antecedent)) == 1):\n",
        "            single_item = set(antecedent).pop()\n",
        "            candidate_set_key = Row(_1=single_item)\n",
        "            subset_k = 1\n",
        "            set(antecedent).add(single_item)\n",
        "        else:\n",
        "            candidate_set_key = tuple(sorted(set(antecedent)))\n",
        "            subset_k = len(set(antecedent))\n",
        "\n",
        "        # support value for the consequent\n",
        "        # value.get helps you to obtain only the value from the key-value pair\n",
        "        antecedent_support = freq_item_sets_map_all_orders_shared[subset_k-1].value.get(candidate_set_key)\n",
        "        \n",
        "        if (antecedent_support is not None):\n",
        "            \n",
        "            # Calculating confidence value for the rule\n",
        "            confidence = freq_item_set_support/antecedent_support\n",
        "        \n",
        "            # Addition of rule if the confidence value is above threshold\n",
        "            if (confidence >= min_confidence):\n",
        "                rules.append((list(antecedent), list(consequent), confidence))      \n",
        "\n",
        "    return rules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCaV8lBfUSNr"
      },
      "source": [
        "**generate_association_rules_for_k_order** \n",
        "\n",
        "It generates association rules for frequent itemsets of order k "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58NZVfW7USNr"
      },
      "source": [
        "def generate_association_rules_for_k_order(k, freq_item_sets_order_k, \\\n",
        "                                           freq_item_sets_map_all_orders_shared, min_confidence):\n",
        "    \"\"\"\n",
        "    Attributes\n",
        "    ----------\n",
        "    \n",
        "    k: order of the frequent item set\n",
        "    \n",
        "    freq_item_sets_map_all_orders_shared: broadcasted map (key-value pair) of frequent item sets where \n",
        "    - key is the subset and \n",
        "    - value is the corresponding support\n",
        "    \n",
        "    min_confidence: Threshold confidence value \n",
        "    \n",
        "    \"\"\"\n",
        "    # Function generate_rules_with_confidence is called as flat map operation over the frequent itemsets of order k\n",
        "    return freq_item_sets_order_k.flatMap(lambda x: generate_rules_with_confidence(x, \\\n",
        "                                  k,freq_item_sets_map_all_orders_shared, \\\n",
        "                                  min_confidence)) \\\n",
        "                                  # Converting the RDD into Dataframe and changing the column names\n",
        "                                 .toDF(('Antecedent', 'Consequent', 'Confidence'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXqHs4O4USNr"
      },
      "source": [
        "**generate_association_rules**\n",
        "\n",
        "It iterates over the all frequent itemsets of different orders and makes a union of association rules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiqgN6NdUSNr"
      },
      "source": [
        "def generate_association_rules(freq_item_sets_all_orders, min_confidence):\n",
        "    \n",
        "    \"\"\"\n",
        "    Attributes\n",
        "    ----------\n",
        "    \n",
        "    freq_item_sets_all_orders: list of all frquent itemsets (calculated above) of different orders\n",
        "    \n",
        "    min_confidence: Threshold confidence value \n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    # As you can see, broadcast function is used here. It is used to broadcast a variable on all the executors.\n",
        "    freq_item_sets_map_all_orders_shared = [spark.sparkContext.broadcast(freq_item_sets.collectAsMap()) \\\n",
        "                                                                     for freq_item_sets in freq_item_sets_all_orders]\n",
        "    \n",
        "    l = len(freq_item_sets_map_all_orders_shared)\n",
        "    \n",
        "    # Calling the function defined above to generate rules from frequent itemset of order 2\n",
        "    # ___________\n",
        "    association_rules_df = generate_association_rules_for_k_order(1, freq_item_sets_all_orders[1], \\\n",
        "                                                                  freq_item_sets_map_all_orders_shared, \\\n",
        "                                                                  min_confidence)\n",
        "    \n",
        "    for k in range(2, l):\n",
        "        if is_freq_item_set_not_empty(freq_item_sets_all_orders[k]):\n",
        "            # Creating union of all association rules for different frequent itemsets\n",
        "            \n",
        "            association_rules_df = association_rules_df.union(generate_association_rules_for_k_order(k, \\\n",
        "                                                        freq_item_sets_all_orders[k], \\\n",
        "                                                        freq_item_sets_map_all_orders_shared, min_confidence))\n",
        "    return association_rules_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERJiKrUEUSNr"
      },
      "source": [
        "# Applying the generate_association_rules function over the list of all frequent itemsets with threshold confidence\n",
        "# score as 0.099 - Values greater than 1 will be used in the marketing strategies\n",
        "\n",
        "association_rules = generate_association_rules(freq_item_sets_all_orders, 0.099)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Meb9-m54USNs"
      },
      "source": [
        "# Printing all the association rules\n",
        "\n",
        "association_rules.show(300, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptlDw2GOUSNs"
      },
      "source": [
        "# Export all the assciation rules in a csv file and copy them to the Excel file provided\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKdnYnqZUSNs"
      },
      "source": [
        "Now you have got the association rules based on the transaction dataset. The next part of the assignment will involve using these rules to calculate the expected revenue from the two strategies"
      ]
    }
  ]
}